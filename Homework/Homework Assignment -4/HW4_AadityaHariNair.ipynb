{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2da2ba-0494-4a20-9eba-cdec104912fe",
   "metadata": {},
   "source": [
    "# Datasets imported\n",
    "DPPS\n",
    "\n",
    "PHYSICAL\n",
    "\n",
    "Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72f2b75-a276-4dfb-9528-356ae4b5a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV files\n",
    "train_X = pd.read_csv('train_X.csv')\n",
    "dpps = pd.read_csv('DPPS.csv', skiprows=2)\n",
    "dpps= dpps.drop('AA_3', axis =1)\n",
    "dpps_used= dpps[[\"AA_1\",\"D7\", \"D8\"]]\n",
    "phys = pd.read_csv(\"Physical.csv\", skiprows=2)\n",
    "phys= phys.drop('AA_3', axis =1)\n",
    "zs = pd.read_csv(\"Z-scale.csv\", skiprows=2)\n",
    "zs= zs.drop('AA_3', axis =1)\n",
    "zs= zs.drop('Z(3)', axis =1)\n",
    "vhse = pd.read_csv(\"VHSE-scale.csv\", skiprows=2)\n",
    "vhse= vhse.drop('AA_3', axis =1)\n",
    "vhse_used= vhse[[\"AA_1\",\"VHSE1\", \"VHSE2\"]]\n",
    "\n",
    "# Clean up the DPPS dataframe\n",
    "dpps_used.columns = [ 'AA_1'] + [f'D{i}' for i in range(7, 9)]\n",
    "dpps_dict = dpps_used.set_index('AA_1').to_dict('index')\n",
    "phys_used= phys[[\"AA_1\",\"Vol\", \"Hydro\"]]\n",
    "phys_dict = phys_used.set_index('AA_1').to_dict('index')\n",
    "zs_dict = zs.set_index('AA_1').to_dict('index')\n",
    "vhse_dict = vhse_used.set_index('AA_1').to_dict('index')\n",
    "\n",
    "def seq_to_dpps(seq):\n",
    "    result = {}\n",
    "    for i, aa in enumerate(seq):\n",
    "        if aa in dpps_dict:\n",
    "            for j in range(7, 9):\n",
    "                result[f'pos_{i+1}_D{j}'] = dpps_dict[aa][f'D{j}']\n",
    "        if aa in phys_dict:\n",
    "            result[f'pos_{i+1}_Phys(Hydro)'] = phys_dict[aa][\"Hydro\"]\n",
    "        if aa in zs_dict:\n",
    "            result[f'pos_{i+1}_Z(1)'] = zs_dict[aa][\"Z(1)\"]\n",
    "            result[f'pos_{i+1}_Z(2)'] = zs_dict[aa][\"Z(2)\"]\n",
    "        if aa in vhse_dict:\n",
    "            result[f'pos_{i+1}_VHSE1'] = vhse_dict[aa][\"VHSE1\"]\n",
    "            result[f'pos_{i+1}_VHSE2'] = vhse_dict[aa][\"VHSE2\"]\n",
    "    return result\n",
    "\n",
    "# Apply the function to each sequence\n",
    "dpps_features = train_X['ConstructedAASeq_cln'].apply(seq_to_dpps)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "\n",
    "# Use 'Id' as the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bfaf62f-58b9-4446-9041-daa37a11969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"train_y.csv\")\n",
    "Y = y.Brightness_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d79a70-fe81-43ae-9967-255cdf511d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 24823\n",
      "Test set size: 6206\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(dpps_features.tolist())\n",
    "\n",
    "# Split the data without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03009dc6-0705-4595-8153-b0549cdb5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9177\n",
      "Test accuracy: 0.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X_train_scaled, y_train)\n",
    "test_score = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ec1ebe0-2596-42cc-bb83-6d3e9bd97c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = pd.read_csv(\"test_X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c1587a5-f3cb-4298-b44c-12349aca746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpps_features_test = test_x['ConstructedAASeq_cln'].apply(seq_to_dpps)\n",
    "id = test_x.Id\n",
    "X_test = pd.DataFrame(dpps_features_test.tolist())\n",
    "X_test = scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "a = {\"Id\": id, \"BrightnessClass\": y_pred}\n",
    "df = pd.DataFrame(a)\n",
    "df.to_csv(\"Submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d29075-761a-4f9c-b373-16d7ca8347be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8652880947600884"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "f1_score(y_test, model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e1fa61-bcd3-4d2d-9d92-befdbf097014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8401559454191033"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1585d513-d3b5-49ca-b1b5-896291748ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934db35-e733-4072-abe9-8510f93cd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'penalty': ['l2', 'elasticnet'],\n",
    "    'solver': [ 'lbfgs', 'liblinear', 'sag'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Create the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(b\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring=f1_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test_scaled, y_test)\n",
    "print(\"Test set F1-score:\", test_score)\n",
    "\n",
    "# If you want to see the detailed results for all parameter combinations\n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"Mean F1-score: {mean_score:.3f} for {params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
