{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2da2ba-0494-4a20-9eba-cdec104912fe",
   "metadata": {},
   "source": [
    "# Datasets imported\n",
    "DPPS\n",
    "\n",
    "PHYSICAL\n",
    "\n",
    "Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72f2b75-a276-4dfb-9528-356ae4b5a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV files\n",
    "train_X = pd.read_csv('train_X.csv')\n",
    "dpps = pd.read_csv('DPPS.csv', skiprows=2)\n",
    "dpps= dpps.drop('AA_3', axis =1)\n",
    "dpps_used= dpps[[\"AA_1\",\"D7\", \"D8\"]]\n",
    "phys = pd.read_csv(\"Physical.csv\", skiprows=2)\n",
    "phys= phys.drop('AA_3', axis =1)\n",
    "zs = pd.read_csv(\"Z-scale.csv\", skiprows=2)\n",
    "zs= zs.drop('AA_3', axis =1)\n",
    "zs= zs.drop('Z(3)', axis =1)\n",
    "vhse = pd.read_csv(\"VHSE-scale.csv\", skiprows=2)\n",
    "vhse= vhse.drop('AA_3', axis =1)\n",
    "vhse_used= vhse[[\"AA_1\",\"VHSE1\", \"VHSE2\"]]\n",
    "mswhim = pd.read_csv(\"MS-WHIM.csv\", skiprows=2)\n",
    "mswhim_used = mswhim[[\"AA_1\", \"2\"]]\n",
    "st = pd.read_csv(\"ST-scale.csv\", skiprows=2)\n",
    "st_used = st[[\"AA_1\", \"ST3\", \"ST4\", \"ST5\"]]\n",
    "\n",
    "# Clean up the DPPS dataframe\n",
    "dpps_used.columns = [ 'AA_1'] + [f'D{i}' for i in range(7, 9)]\n",
    "dpps_dict = dpps_used.set_index('AA_1').to_dict('index')\n",
    "phys_used= phys[[\"AA_1\",\"Vol\", \"Hydro\"]]\n",
    "phys_dict = phys_used.set_index('AA_1').to_dict('index')\n",
    "zs_dict = zs.set_index('AA_1').to_dict('index')\n",
    "vhse_dict = vhse_used.set_index('AA_1').to_dict('index')\n",
    "mswhim_dict = mswhim_used.set_index('AA_1').to_dict('index')\n",
    "st_dict = st_used.set_index('AA_1').to_dict('index')\n",
    "def seq_to_dpps(seq):\n",
    "    result = {}\n",
    "    for i, aa in enumerate(seq):\n",
    "        if aa in dpps_dict:\n",
    "            for j in range(7, 9):\n",
    "                result[f'pos_{i+1}_D{j}'] = dpps_dict[aa][f'D{j}']\n",
    "        if aa in phys_dict:\n",
    "            result[f'pos_{i+1}_Phys(Hydro)'] = phys_dict[aa][\"Hydro\"]\n",
    "        if aa in zs_dict:\n",
    "            result[f'pos_{i+1}_Z(1)'] = zs_dict[aa][\"Z(1)\"]\n",
    "            result[f'pos_{i+1}_Z(2)'] = zs_dict[aa][\"Z(2)\"]\n",
    "        if aa in vhse_dict:\n",
    "            result[f'pos_{i+1}_VHSE1'] = vhse_dict[aa][\"VHSE1\"]\n",
    "            result[f'pos_{i+1}_VHSE2'] = vhse_dict[aa][\"VHSE2\"]\n",
    "        if aa in mswhim_dict:\n",
    "            result[f'pos_{i+1}_MSWHIM1'] = mswhim_dict[aa][\"2\"]\n",
    "        if aa in st_dict:\n",
    "            for j in range(3, 6):\n",
    "                result[f'pos_{i+1}_ST{j}'] = st_dict[aa][f'ST{j}']\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Apply the function to each sequence\n",
    "dpps_features = train_X['ConstructedAASeq_cln'].apply(seq_to_dpps)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "\n",
    "# Use 'Id' as the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfaf62f-58b9-4446-9041-daa37a11969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"train_y.csv\")\n",
    "Y = y.Brightness_Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d79a70-fe81-43ae-9967-255cdf511d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 27926\n",
      "Test set size: 3103\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(dpps_features.tolist())\n",
    "\n",
    "# Split the data without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03009dc6-0705-4595-8153-b0549cdb5c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9218\n",
      "Test accuracy: 0.8959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X_train_scaled, y_train)\n",
    "test_score = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0d29075-761a-4f9c-b373-16d7ca8347be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8687525396180414"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "f1_score(y_test, model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97e1fa61-bcd3-4d2d-9d92-befdbf097014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8531524341580208"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, model.predict(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ec1ebe0-2596-42cc-bb83-6d3e9bd97c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = pd.read_csv(\"test_X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c1587a5-f3cb-4298-b44c-12349aca746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpps_features_test = test_x['ConstructedAASeq_cln'].apply(seq_to_dpps)\n",
    "id = test_x.Id\n",
    "X_test = pd.DataFrame(dpps_features_test.tolist())\n",
    "X_test = scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "a = {\"Id\": id, \"BrightnessClass\": y_pred}\n",
    "df = pd.DataFrame(a)\n",
    "df.to_csv(\"Submit.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1585d513-d3b5-49ca-b1b5-896291748ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv(\"Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d743da1-dfe2-4346-9346-7d63a803f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fca11f-fa93-47bf-8225-060958c827d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Define meta-classifier\n",
    "meta_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create custom scorer that combines accuracy and precision\n",
    "def custom_scorer(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    return (accuracy + precision) / 2\n",
    "\n",
    "custom_scorer = make_scorer(custom_scorer)\n",
    "\n",
    "# Create the StackingClassifier\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_classifier,\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the stacking classifier\n",
    "stacking_scores = cross_val_score(stacking_classifier, X, y, cv=5, scoring=custom_scorer)\n",
    "print(f\"Stacking Classifier Average Score: {stacking_scores.mean():.4f} (+/- {stacking_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Compare with individual base models\n",
    "for name, model in base_models:\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring=custom_scorer)\n",
    "    print(f\"{name.upper()} Average Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e3eeba-8383-43c8-a3c3-b24a3b66fa7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stacking_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m stacking_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stacking_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = stacking_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare with individual base models\n",
    "for name, model in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name.upper()} Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
